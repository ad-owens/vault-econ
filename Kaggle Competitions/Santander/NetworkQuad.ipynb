{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation as cv\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "###From here on a is a function of the weights and biases (w,b) and input x, a=a(x,w,b).\n",
    "###p is the penalty that corresponds to an anistropic learning rate for the minority and \n",
    "##majority class. y is the target output. delta is the derivative of the cost function,  \n",
    "##z^l = w^l a^{l-1} + b^l is the weighted input for each new layer of neurons, \n",
    "##i.e. a^l = sigmoid(z^l).\n",
    "##lambda is the regularization parameter.\n",
    "class QuadraticCost(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y, p):\n",
    "        if y == 1: return p*0.5*np.linalg.norm(a-y)**2\n",
    "        else: return 0.5*np.linalg.norm(a-y)**2\n",
    "        \n",
    "# delta^l = grad_a^l(cost) * sigma'(z^l) is the local layer l's rate of cost change \n",
    "# that is propagated back to the input in the backpropagation algorithm \n",
    "    @staticmethod\n",
    "    def delta(z, a, y, p):\n",
    "        if y == 1: return p*(a-y)*sigmoid_prime(z)\n",
    "        return (a-y)*sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y, p):\n",
    "        if y == 1: return p*np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "        else: return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "        \n",
    "##z is not used by this method but we include it in the parameters for the sake of cohesion.\n",
    "    @staticmethod\n",
    "    def delta(z, a, y, p):\n",
    "        if y == 1: return p*(a-y)\n",
    "        else: return (a-y)\n",
    "\n",
    "\n",
    "#### THE NETWORK CLASS\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, previous_w, previous_b, cost=QuadraticCost):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes # number of neurons in the respective layers of the network\n",
    "        self.cost = cost\n",
    "        #self.previous_w = previous_w\n",
    "        #self.previous_b = previous_b\n",
    "        if previous_w == [] or previous_b == []:\n",
    "            print \"Default weights\"\n",
    "            self.default_weight_initializer()\n",
    "        else:\n",
    "            print \"Best previous weights\"\n",
    "            self.previous_best_weight(previous_w, previous_b)\n",
    "        \n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "#initialize the algorithm with the best weights from a previous run        \n",
    "    def previous_best_weight(self, previous_w, previous_b):\n",
    "        self.weights = previous_w\n",
    "        self.biases = previous_b\n",
    "        \n",
    "#standard feedforward algorithm for the neural net. sigmoid is vectorized and returns a\n",
    "#for each hidden layer.\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "#stochastic gradient descent method uses a sequence of subsetss \"minibatch\" of the data \n",
    "#to get an estimate for the cost gradient. optional parameters will monitor the \n",
    "#set cost, accuracy for both training and evaluation sets.\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, penalty,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False,\n",
    "            monitor_roc=False,\n",
    "            monitor_roc_test=False):\n",
    "        \n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        self.evaluation_cost, evaluation_accuracy = [], []\n",
    "        self.training_cost, self.training_accuracy = [], []\n",
    "        self.training_data, self.training_roc = [], []\n",
    "        self.testing_roc = []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_weights_bias(\n",
    "                    mini_batch, eta, lmbda, len(training_data), penalty)\n",
    "            #print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda, penalty)\n",
    "                self.training_cost.append(cost)\n",
    "                #print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                self.training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_roc:\n",
    "                roc = self.rocauc(training_data)\n",
    "                self.training_roc.append(roc)\n",
    "                #print \"Roc on training data: {}\".format(roc)\n",
    "            if monitor_roc_test:\n",
    "                roc = self.rocauc(evaluation_data)\n",
    "                self.testing_roc.append(roc)\n",
    "                #print \"Roc on testing data: {}\".format(roc)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, penalty)\n",
    "                self.evaluation_cost.append(cost)\n",
    "                #print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                #print \"Accuracy on evaluation data: {} / {}\".format( \\\n",
    "                #    self.accuracy(evaluation_data), n_data)\n",
    "            #print\n",
    "        return self.evaluation_cost, evaluation_accuracy, \\\n",
    "               self.training_cost, self.training_accuracy, \\\n",
    "               self.training_data, self.training_roc, \\\n",
    "               self.weights, self.biases\n",
    "\n",
    "#this calls on the backpropagation method to update the weights and biases for each layer\n",
    "#(note the first layer is the input and has no biases). at each step we send (w,b) to\n",
    "#(w - eta*[nabla_w(cost) + (lambda/n)*w], b - eta*nabla_b(cost))\n",
    "    def update_weights_bias(self, mini_batch, eta, lmbda, n, pen):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y, pen)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                        for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "#backpropagation algorithm has an extra parameter penalty=pen which depends on the proportion \n",
    "#of the minority class 1. it boils down to using a faster learning rate on the 1s than on \n",
    "#the 0s.  the class importance of the 1s is implicitly included in the cost function, since\n",
    "#these multiplicative importances show up in the gradient of the cost. backprop(x,y,pen)\n",
    "#rturns a tuple of vectors (nabla_b,nabla_w) representing the gradient nabla_(b,w)C.\n",
    "    def backprop(self, x, y, pen):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y, pen)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # backward pass\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l] # we use the same last z vector to compute all nabla_b/w\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        \"\"\"\n",
    "        results = [(round(self.feedforward(x)), y) for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "    \n",
    "    def rocauc(self, data):\n",
    "        pred = [self.feedforward(x)[0][0] for (x, y) in data]\n",
    "        target = [y for (x, y) in data]\n",
    "        return roc_auc_score(target, pred)\n",
    "    \n",
    "    \n",
    "    def get_data(self, data):\n",
    "        return self.data\n",
    "\n",
    "    \n",
    "    def predict(self, data):\n",
    "        result = [self.feedforward(x)[0][0] for x in data]\n",
    "        return result\n",
    "    \n",
    "    def train_cost(self, data):\n",
    "        return self.training_cost\n",
    "    \n",
    "    def evaluate_cost(self, data):\n",
    "        return self.evaluation_cost\n",
    "    \n",
    "    def train_roc(self, data):\n",
    "        return self.training_roc\n",
    "    \n",
    "    def test_roc(self, data):\n",
    "        return self.testing_roc\n",
    "    \n",
    "    def trainning_data(self):\n",
    "        return self.training_data\n",
    "    \n",
    "    def train_acc(self, data):\n",
    "        return self.training_accuracy\n",
    "\n",
    "#function to monitor the total cost\n",
    "    def total_cost(self, data, lmbda, penalty):\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            cost += self.cost.fn(a, y, penalty)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "    \n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Read train and test\n",
    "#####################\n",
    "train = pd.read_csv(\"/Users/boulenge/Desktop/Projects/Project4 - ML/train.csv\", sep=',',\\\n",
    "                        encoding='utf-8')\n",
    "train = train.set_index(\"ID\")\n",
    "test = pd.read_csv(\"/Users/boulenge/Desktop/Projects/Project4 - ML/test.csv\", sep=',',\\\n",
    "                        encoding='utf-8')\n",
    "test = test.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full_train shape before dropping similar features: (76020, 369)\n",
      "Full_train shape after: (76020, 321)\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Remove the similar features\n",
    "#############################\n",
    "Full_train = train.iloc[:, :369]\n",
    "temp = Full_train\n",
    "ind_same = {}\n",
    "for i in range(len(temp.columns)):\n",
    "    tp = temp.drop(temp.columns[i], inplace=False, axis=1)\n",
    "    for j in range(i+1, len(tp.columns)):\n",
    "        lst = reduce(lambda x, y: x + y, ind_same.values(), [])\n",
    "        if all(temp.iloc[:, i] == tp.iloc[:, j]):\n",
    "            if i not in ind_same.keys():\n",
    "                if i not in lst:\n",
    "                    if j >= i: ind_same[i] = [j+1]\n",
    "                    else: ind_same[i] = [j]\n",
    "            else:\n",
    "                if j >= i: ind_same[i].append(j+1)\n",
    "                else: ind_same[i].append(j)\n",
    "\n",
    "print \"Full_train shape before dropping similar features: \" + str(Full_train.shape)\n",
    "ind_drop = reduce(lambda x, y: x + y, ind_same.values(), [])                 \n",
    "Full_train.drop(Full_train.columns[ind_drop], inplace=True, axis=1)\n",
    "print \"Full_train shape after: \" + str(Full_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (57015, 321)(57015,)\n",
      "Test: (19005, 321)(19005,)\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "### Cross-Validation\n",
    "#############################################\n",
    "# Cross-Validation and evaluate_model\n",
    "train_target = train.iloc[:, 369]\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "             cv.train_test_split(Full_train, train_target, test_size=0.25, random_state=0, \\\n",
    "                                 stratify = train_target)\n",
    "\n",
    "def evaluate_model(clf):\n",
    "    \"\"\"Scores a model using log loss with the created train and test sets.\"\"\"\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print \"Train score:\", sklearn.metrics.roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n",
    "    print \"Test score:\", sklearn.metrics.roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    print \"Total time:\", time.time() - start\n",
    "\n",
    "print \"Training: \" + str(X_train.shape) + str(y_train.shape)\n",
    "print \"Test: \" + str(X_test.shape) + str(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Normalize train and test\n",
    "#############################################\n",
    "# Normalize train (with no target)\n",
    "Full_train = train.iloc[:, :369]\n",
    "Full_train = min_max_scaler.fit_transform(Full_train)\n",
    "Full_train = pd.DataFrame(Full_train, index = train.index)\n",
    "#Full_train.to_csv(\"/Users/boulenge/Desktop/Projects/Project4 - ML/Full_train.csv\")\n",
    "train_target = train.iloc[:, 369]\n",
    "\n",
    "# Normalize test\n",
    "Full_test = test\n",
    "Full_test = min_max_scaler.fit_transform(Full_test)\n",
    "Full_test = pd.DataFrame(Full_test, index = test.index)\n",
    "#Full_test.to_csv(\"/Users/boulenge/Desktop/Projects/Project4 - ML/Full_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 useless features\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "##USING RANDOM FOREST TO WEED OUT ZERO VARIANCE FEATURES\n",
    "########################################################\n",
    "randomForest = ensemble.RandomForestClassifier(n_estimators=50, class_weight = 'balanced', \\\n",
    "                                               max_depth =10, oob_score=True)\n",
    "randomForest.fit(X_train, y_train)\n",
    "\n",
    "print str(np.sum(randomForest.feature_importances_==0)) + \" useless features\"\n",
    "\n",
    "#remove unimportant features by index\n",
    "feature_imprtance = zip(Full_train.columns, randomForest.feature_importances_)\n",
    "dtype = [('feature', 'S10'), ('importance', 'float')]\n",
    "feature_imprtance = np.array(feature_imprtance, dtype = dtype)\n",
    "feature_sort = np.sort(feature_imprtance, order='importance')[::-1]\n",
    "imp = np.sort([int(i) for (i, j) in feature_sort if j!=0])\n",
    "useless = [int(i) for (i, j) in feature_sort if j==0]\n",
    "#filtered_train = Full_train.iloc[:, imp]\n",
    "#filtered_train.to_csv('/Users/boulenge/Desktop/Projects/Project4 - ML/filtered_train.csv')\n",
    "\n",
    "# filtering out the useless features in Full_train, train_target and Full_test\n",
    "Full_train = Full_train.iloc[:, imp]\n",
    "Full_test = Full_test.iloc[:, imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Convert the training and target data\n",
    "#############################################\n",
    "trainarray = [np.reshape(x, (len(Full_train.columns), 1)) for x in Full_train.values]\n",
    "target = train.iloc[:,369].values.T\n",
    "\n",
    "# Convert the testing data\n",
    "testarray = [np.reshape(x, (len(Full_test.columns), 1)) for x in Full_test.values]\n",
    "\n",
    "# Full training data\n",
    "training_data = zip(trainarray, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Simple Neural Net instance\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default weights\n",
      "step completed: 1 / 288\n",
      "Default weights\n",
      "step completed: 2 / 288\n",
      "Default weights\n",
      "step completed: 3 / 288\n",
      "Default weights\n",
      "step completed: 4 / 288\n",
      "Default weights\n",
      "step completed: 5 / 288\n",
      "Default weights\n",
      "step completed: 6 / 288\n",
      "Default weights\n",
      "step completed: 7 / 288\n",
      "Default weights\n",
      "step completed: 8 / 288\n",
      "Default weights\n",
      "step completed: 9 / 288\n",
      "Default weights\n",
      "step completed: 10 / 288\n",
      "Default weights\n",
      "step completed: 11 / 288\n",
      "Default weights\n",
      "step completed: 12 / 288\n",
      "Default weights\n",
      "step completed: 13 / 288\n",
      "Default weights\n",
      "step completed: 14 / 288\n",
      "Default weights\n",
      "step completed: 15 / 288\n",
      "Default weights\n",
      "step completed: 16 / 288\n",
      "Default weights\n",
      "step completed: 17 / 288\n",
      "Default weights\n",
      "step completed: 18 / 288\n",
      "Default weights\n",
      "step completed: 19 / 288\n",
      "Default weights\n",
      "step completed: 20 / 288\n",
      "Default weights\n",
      "step completed: 21 / 288\n",
      "Default weights\n",
      "step completed: 22 / 288\n",
      "Default weights\n",
      "step completed: 23 / 288\n",
      "Default weights\n",
      "step completed: 24 / 288\n",
      "Default weights\n",
      "step completed: 25 / 288\n",
      "Default weights\n",
      "step completed: 26 / 288\n",
      "Default weights\n",
      "step completed: 27 / 288\n",
      "Default weights\n",
      "step completed: 28 / 288\n",
      "Default weights\n",
      "step completed: 29 / 288\n",
      "Default weights\n",
      "step completed: 30 / 288\n",
      "Default weights\n",
      "step completed: 31 / 288\n",
      "Default weights\n",
      "step completed: 32 / 288\n",
      "Default weights\n",
      "step completed: 33 / 288\n",
      "Default weights\n",
      "step completed: 34 / 288\n",
      "Default weights\n",
      "step completed: 35 / 288\n",
      "Default weights\n",
      "step completed: 36 / 288\n",
      "Default weights\n",
      "step completed: 37 / 288\n",
      "Default weights\n",
      "step completed: 38 / 288\n",
      "Default weights\n",
      "step completed: 39 / 288\n",
      "Default weights\n",
      "step completed: 40 / 288\n",
      "Default weights\n",
      "step completed: 41 / 288\n",
      "Default weights\n",
      "step completed: 42 / 288\n",
      "Default weights\n",
      "step completed: 43 / 288\n",
      "Default weights\n",
      "step completed: 44 / 288\n",
      "Default weights\n",
      "step completed: 45 / 288\n",
      "Default weights\n",
      "step completed: 46 / 288\n",
      "Default weights\n",
      "step completed: 47 / 288\n",
      "Default weights\n",
      "step completed: 48 / 288\n",
      "Default weights\n",
      "step completed: 49 / 288\n",
      "Default weights\n",
      "step completed: 50 / 288\n",
      "Default weights\n",
      "step completed: 51 / 288\n",
      "Default weights\n",
      "step completed: 52 / 288\n",
      "Default weights\n",
      "step completed: 53 / 288\n",
      "Default weights\n",
      "step completed: 54 / 288\n",
      "Default weights\n",
      "step completed: 55 / 288\n",
      "Default weights\n",
      "step completed: 56 / 288\n",
      "Default weights\n",
      "step completed: 57 / 288\n",
      "Default weights\n",
      "step completed: 58 / 288\n",
      "Default weights\n",
      "step completed: 59 / 288\n",
      "Default weights\n",
      "step completed: 60 / 288\n",
      "Default weights\n",
      "step completed: 61 / 288\n",
      "Default weights\n",
      "step completed: 62 / 288\n",
      "Default weights\n",
      "step completed: 63 / 288\n",
      "Default weights\n",
      "step completed: 64 / 288\n",
      "Default weights\n",
      "step completed: 65 / 288\n",
      "Default weights\n",
      "step completed: 66 / 288\n",
      "Default weights\n",
      "step completed: 67 / 288\n",
      "Default weights\n",
      "step completed: 68 / 288\n",
      "Default weights\n",
      "step completed: 69 / 288\n",
      "Default weights\n",
      "step completed: 70 / 288\n",
      "Default weights\n",
      "step completed: 71 / 288\n",
      "Default weights\n",
      "step completed: 72 / 288\n",
      "Default weights\n",
      "step completed: 73 / 288\n",
      "Default weights\n",
      "step completed: 74 / 288\n",
      "Default weights\n",
      "step completed: 75 / 288\n",
      "Default weights\n",
      "step completed: 76 / 288\n",
      "Default weights\n",
      "step completed: 77 / 288\n",
      "Default weights\n",
      "step completed: 78 / 288\n",
      "Default weights\n",
      "step completed: 79 / 288\n",
      "Default weights\n",
      "step completed: 80 / 288\n",
      "Default weights\n",
      "step completed: 81 / 288\n",
      "Default weights\n",
      "step completed: 82 / 288\n",
      "Default weights\n",
      "step completed: 83 / 288\n",
      "Default weights\n",
      "step completed: 84 / 288\n",
      "Default weights\n",
      "step completed: 85 / 288\n",
      "Default weights\n",
      "step completed: 86 / 288\n",
      "Default weights\n",
      "step completed: 87 / 288\n",
      "Default weights\n",
      "step completed: 88 / 288\n",
      "Default weights\n",
      "step completed: 89 / 288\n",
      "Default weights\n",
      "step completed: 90 / 288\n",
      "Default weights\n",
      "step completed: 91 / 288\n",
      "Default weights\n",
      "step completed: 92 / 288\n",
      "Default weights\n",
      "step completed: 93 / 288\n",
      "Default weights\n",
      "step completed: 94 / 288\n",
      "Default weights\n",
      "step completed: 95 / 288\n",
      "Default weights\n",
      "step completed: 96 / 288\n",
      "Default weights\n",
      "step completed: 97 / 288\n",
      "Default weights\n",
      "step completed: 98 / 288\n",
      "Default weights\n",
      "step completed: 99 / 288\n",
      "Default weights\n",
      "step completed: 100 / 288\n",
      "Default weights\n",
      "step completed: 101 / 288\n",
      "Default weights\n",
      "step completed: 102 / 288\n",
      "Default weights\n",
      "step completed: 103 / 288\n",
      "Default weights\n",
      "step completed: 104 / 288\n",
      "Default weights\n",
      "step completed: 105 / 288\n",
      "Default weights\n",
      "step completed: 106 / 288\n",
      "Default weights\n",
      "step completed: 107 / 288\n",
      "Default weights\n",
      "step completed: 108 / 288\n",
      "Default weights\n",
      "step completed: 109 / 288\n",
      "Default weights\n",
      "step completed: 110 / 288\n",
      "Default weights\n",
      "step completed: 111 / 288\n",
      "Default weights\n",
      "step completed: 112 / 288\n",
      "Default weights\n",
      "step completed: 113 / 288\n",
      "Default weights\n",
      "step completed: 114 / 288\n",
      "Default weights\n",
      "step completed: 115 / 288\n",
      "Default weights\n",
      "step completed: 116 / 288\n",
      "Default weights\n",
      "step completed: 117 / 288\n",
      "Default weights\n",
      "step completed: 118 / 288\n",
      "Default weights\n",
      "step completed: 119 / 288\n",
      "Default weights\n",
      "step completed: 120 / 288\n",
      "Default weights\n",
      "step completed: 121 / 288\n",
      "Default weights\n",
      "step completed: 122 / 288\n",
      "Default weights\n",
      "step completed: 123 / 288\n",
      "Default weights\n",
      "step completed: 124 / 288\n",
      "Default weights\n",
      "step completed: 125 / 288\n",
      "Default weights\n",
      "step completed: 126 / 288\n",
      "Default weights\n",
      "step completed: 127 / 288\n",
      "Default weights\n",
      "step completed: 128 / 288\n",
      "Default weights\n",
      "step completed: 129 / 288\n",
      "Default weights\n",
      "step completed: 130 / 288\n",
      "Default weights\n",
      "step completed: 131 / 288\n",
      "Default weights\n",
      "step completed: 132 / 288\n",
      "Default weights\n",
      "step completed: 133 / 288\n",
      "Default weights\n",
      "step completed: 134 / 288\n",
      "Default weights\n",
      "step completed: 135 / 288\n",
      "Default weights\n",
      "step completed: 136 / 288\n",
      "Default weights\n",
      "step completed: 137 / 288\n",
      "Default weights\n",
      "step completed: 138 / 288\n",
      "Default weights\n",
      "step completed: 139 / 288\n",
      "Default weights\n",
      "step completed: 140 / 288\n",
      "Default weights\n",
      "step completed: 141 / 288\n",
      "Default weights\n",
      "step completed: 142 / 288\n",
      "Default weights\n",
      "step completed: 143 / 288\n",
      "Default weights\n",
      "step completed: 144 / 288\n",
      "Default weights\n",
      "step completed: 145 / 288\n",
      "Default weights\n",
      "step completed: 146 / 288\n",
      "Default weights\n",
      "step completed: 147 / 288\n",
      "Default weights\n",
      "step completed: 148 / 288\n",
      "Default weights\n",
      "step completed: 149 / 288\n",
      "Default weights\n",
      "step completed: 150 / 288\n",
      "Default weights\n",
      "step completed: 151 / 288\n",
      "Default weights\n",
      "step completed: 152 / 288\n",
      "Default weights\n",
      "step completed: 153 / 288\n",
      "Default weights\n",
      "step completed: 154 / 288\n",
      "Default weights\n",
      "step completed: 155 / 288\n",
      "Default weights\n",
      "step completed: 156 / 288\n",
      "Default weights\n",
      "step completed: 157 / 288\n",
      "Default weights\n",
      "step completed: 158 / 288\n",
      "Default weights\n",
      "step completed: 159 / 288\n",
      "Default weights\n",
      "step completed: 160 / 288\n",
      "Default weights\n",
      "step completed: 161 / 288\n",
      "Default weights\n",
      "step completed: 162 / 288\n",
      "Default weights\n",
      "step completed: 163 / 288\n",
      "Default weights\n",
      "step completed: 164 / 288\n",
      "Default weights\n",
      "step completed: 165 / 288\n",
      "Default weights\n",
      "step completed: 166 / 288\n",
      "Default weights\n",
      "step completed: 167 / 288\n",
      "Default weights\n",
      "step completed: 168 / 288\n",
      "Default weights\n",
      "step completed: 169 / 288\n",
      "Default weights\n",
      "step completed: 170 / 288\n",
      "Default weights\n",
      "step completed: 171 / 288\n",
      "Default weights\n",
      "step completed: 172 / 288\n",
      "Default weights\n",
      "step completed: 173 / 288\n",
      "Default weights\n",
      "step completed: 174 / 288\n",
      "Default weights\n",
      "step completed: 175 / 288\n",
      "Default weights\n",
      "step completed: 176 / 288\n",
      "Default weights\n",
      "step completed: 177 / 288\n",
      "Default weights\n",
      "step completed: 178 / 288\n",
      "Default weights\n",
      "step completed: 179 / 288\n",
      "Default weights\n",
      "step completed: 180 / 288\n",
      "Default weights\n",
      "step completed: 181 / 288\n",
      "Default weights\n",
      "step completed: 182 / 288\n",
      "Default weights\n",
      "step completed: 183 / 288\n",
      "Default weights\n",
      "step completed: 184 / 288\n",
      "Default weights\n",
      "step completed: 185 / 288\n",
      "Default weights\n",
      "step completed: 186 / 288\n",
      "Default weights\n",
      "step completed: 187 / 288\n",
      "Default weights\n",
      "step completed: 188 / 288\n",
      "Default weights\n",
      "step completed: 189 / 288\n",
      "Default weights\n",
      "step completed: 190 / 288\n",
      "Default weights\n",
      "step completed: 191 / 288\n",
      "Default weights\n",
      "step completed: 192 / 288\n",
      "Default weights\n",
      "step completed: 193 / 288\n",
      "Default weights\n",
      "step completed: 194 / 288\n",
      "Default weights\n",
      "step completed: 195 / 288\n",
      "Default weights\n",
      "step completed: 196 / 288\n",
      "Default weights\n",
      "step completed: 197 / 288\n",
      "Default weights\n",
      "step completed: 198 / 288\n",
      "Default weights\n",
      "step completed: 199 / 288\n",
      "Default weights\n",
      "step completed: 200 / 288\n",
      "Default weights\n",
      "step completed: 201 / 288\n",
      "Default weights\n",
      "step completed: 202 / 288\n",
      "Default weights\n",
      "step completed: 203 / 288\n",
      "Default weights\n",
      "step completed: 204 / 288\n",
      "Default weights\n",
      "step completed: 205 / 288\n",
      "Default weights\n",
      "step completed: 206 / 288\n",
      "Default weights\n",
      "step completed: 207 / 288\n",
      "Default weights\n",
      "step completed: 208 / 288\n",
      "Default weights\n",
      "step completed: 209 / 288\n",
      "Default weights\n",
      "step completed: 210 / 288\n",
      "Default weights\n",
      "step completed: 211 / 288\n",
      "Default weights\n",
      "step completed: 212 / 288\n",
      "Default weights\n",
      "step completed: 213 / 288\n",
      "Default weights\n",
      "step completed: 214 / 288\n",
      "Default weights\n",
      "step completed: 215 / 288\n",
      "Default weights\n",
      "step completed: 216 / 288\n",
      "Default weights\n",
      "step completed: 217 / 288\n",
      "Default weights\n",
      "step completed: 218 / 288\n",
      "Default weights\n",
      "step completed: 219 / 288\n",
      "Default weights\n",
      "step completed: 220 / 288\n",
      "Default weights\n",
      "step completed: 221 / 288\n",
      "Default weights\n",
      "step completed: 222 / 288\n",
      "Default weights\n",
      "step completed: 223 / 288\n",
      "Default weights\n",
      "step completed: 224 / 288\n",
      "Default weights\n",
      "step completed: 225 / 288\n",
      "Default weights\n",
      "step completed: 226 / 288\n",
      "Default weights\n",
      "step completed: 227 / 288\n",
      "Default weights\n",
      "step completed: 228 / 288\n",
      "Default weights\n",
      "step completed: 229 / 288\n",
      "Default weights\n",
      "step completed: 230 / 288\n",
      "Default weights\n",
      "step completed: 231 / 288\n",
      "Default weights\n",
      "step completed: 232 / 288\n",
      "Default weights\n",
      "step completed: 233 / 288\n",
      "Default weights\n",
      "step completed: 234 / 288\n",
      "Default weights\n",
      "step completed: 235 / 288\n",
      "Default weights\n",
      "step completed: 236 / 288\n",
      "Default weights\n",
      "step completed: 237 / 288\n",
      "Default weights\n",
      "step completed: 238 / 288\n",
      "Default weights\n",
      "step completed: 239 / 288\n",
      "Default weights\n",
      "step completed: 240 / 288\n",
      "Default weights\n",
      "step completed: 241 / 288\n",
      "Default weights\n",
      "step completed: 242 / 288\n",
      "Default weights\n",
      "step completed: 243 / 288\n",
      "Default weights\n",
      "step completed: 244 / 288\n",
      "Default weights\n",
      "step completed: 245 / 288\n",
      "Default weights\n",
      "step completed: 246 / 288\n",
      "Default weights\n",
      "step completed: 247 / 288\n",
      "Default weights\n",
      "step completed: 248 / 288\n",
      "Default weights\n",
      "step completed: 249 / 288\n",
      "Default weights\n",
      "step completed: 250 / 288\n",
      "Default weights\n",
      "step completed: 251 / 288\n",
      "Default weights\n",
      "step completed: 252 / 288\n",
      "Default weights\n",
      "step completed: 253 / 288\n",
      "Default weights\n",
      "step completed: 254 / 288\n",
      "Default weights\n",
      "step completed: 255 / 288\n",
      "Default weights\n",
      "step completed: 256 / 288\n",
      "Default weights\n",
      "step completed: 257 / 288\n",
      "Default weights\n",
      "step completed: 258 / 288\n",
      "Default weights\n",
      "step completed: 259 / 288\n",
      "Default weights\n",
      "step completed: 260 / 288\n",
      "Default weights\n",
      "step completed: 261 / 288\n",
      "Default weights\n",
      "step completed: 262 / 288\n",
      "Default weights\n",
      "step completed: 263 / 288\n",
      "Default weights\n",
      "step completed: 264 / 288\n",
      "Default weights\n",
      "step completed: 265 / 288\n",
      "Default weights\n",
      "step completed: 266 / 288\n",
      "Default weights\n",
      "step completed: 267 / 288\n",
      "Default weights\n",
      "step completed: 268 / 288\n",
      "Default weights\n",
      "step completed: 269 / 288\n",
      "Default weights\n",
      "step completed: 270 / 288\n",
      "Default weights\n",
      "step completed: 271 / 288\n",
      "Default weights\n",
      "step completed: 272 / 288\n",
      "Default weights\n",
      "step completed: 273 / 288\n",
      "Default weights\n",
      "step completed: 274 / 288\n",
      "Default weights\n",
      "step completed: 275 / 288\n",
      "Default weights\n",
      "step completed: 276 / 288\n",
      "Default weights\n",
      "step completed: 277 / 288\n",
      "Default weights\n",
      "step completed: 278 / 288\n",
      "Default weights\n",
      "step completed: 279 / 288\n",
      "Default weights\n",
      "step completed: 280 / 288\n",
      "Default weights\n",
      "step completed: 281 / 288\n",
      "Default weights\n",
      "step completed: 282 / 288\n",
      "Default weights\n",
      "step completed: 283 / 288\n",
      "Default weights\n",
      "step completed: 284 / 288\n",
      "Default weights\n",
      "step completed: 285 / 288\n",
      "Default weights\n",
      "step completed: 286 / 288\n",
      "Default weights\n",
      "step completed: 287 / 288\n",
      "Default weights\n",
      "step completed: 288 / 288\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('object') and format specifier ('%.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-db9acb4ee6b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mdf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_res.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"previous_best_weights.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"previous_best_biases.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boulenge/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[1;32m   1159\u001b[0m                                     \u001b[0;34m\"format specifier ('%s')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m                                     % (str(X.dtype), format))\n\u001b[0m\u001b[1;32m   1161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0mfooter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfooter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('object') and format specifier ('%.18e')"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "### GridSearch for tuning parameters: averaging multiple times ###\n",
    "##################################################################\n",
    "\n",
    "# organizing the results\n",
    "df_res = pd.DataFrame({'penalty': [], 'lmbda': [], 'eta': [], \\\n",
    "                       'roc_train': [], 'cost_train': [], \\\n",
    "                       'roc_test': [], 'cost_test': [], 'run_id': [], \\\n",
    "                       'roc_test_avg': [], 'roc_train_avg': [], 'roc_test_avg_run': []})\n",
    "\n",
    "# Reload train to get lines indices\n",
    "train_line = pd.read_csv(\"/Users/boulenge/Desktop/Projects/Project4 - ML/train.csv\", sep=',',\\\n",
    "                        encoding='utf-8')\n",
    "\n",
    "# size of the Training sample\n",
    "N_samp = 2000\n",
    "# size of the Testing sample\n",
    "N_samp_test = 2000\n",
    "\n",
    "#Pen = np.arange(16, 20, 1)\n",
    "#Lmbda = np.arange(6, 25, 4)\n",
    "#Eta = [1e-4, 1e-3, 1e-2]\n",
    "#Run_count = range(1, 5)\n",
    "\n",
    "Pen = np.arange(16, 20, 1)\n",
    "Lmbda = np.arange(8, 20, 2)\n",
    "Eta = [1e-4, 1e-3, 1e-2]\n",
    "Run_count = range(1, 5)\n",
    "\n",
    "\n",
    "result, best_roc, best_cost = [], [], []\n",
    "count = 1\n",
    "Nb_step = len(Pen)*len(Lmbda)*len(Eta)*len(Run_count)\n",
    "\n",
    "run_count = 1\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "roc_avg = 0\n",
    "\n",
    "for run_c in Run_count:\n",
    "    ind_samp_train = random.sample(range(len(Full_train)), N_samp)\n",
    "    train_temp = Full_train.iloc[ind_samp_train, :]\n",
    "    \n",
    "    out_train = [i for i in range(len(Full_train)) if i not in ind_samp_train]\n",
    "    ind_samp_test = random.sample(out_train, N_samp_test)\n",
    "    test_temp = Full_train.iloc[ind_samp_test, :]\n",
    "    \n",
    "    # Neural input\n",
    "    train_array = [np.reshape(x, (len(train_temp.columns), 1)) for x in train_temp.values]\n",
    "    target_train = train.iloc[ind_samp_train, 369]\n",
    "    training_data_r = zip(train_array, target_train)\n",
    "    \n",
    "    test_array = [np.reshape(x, (len(test_temp.columns), 1)) for x in test_temp.values]\n",
    "    target_test = train.iloc[ind_samp_test, 369]\n",
    "    testing_data_r = zip(test_array, target_test)\n",
    "    \n",
    "    for pen in Pen:\n",
    "        for lmbda in Lmbda:\n",
    "            for eta in Eta:\n",
    "                net_r = Network([len(training_data_r[0][0]), 100, 10, 1], [], [])\n",
    "                net_r.SGD(training_data=training_data_r, \\\n",
    "                    evaluation_data = testing_data_r, \\\n",
    "                    epochs=2, mini_batch_size=500, \\\n",
    "                    eta=eta, lmbda = lmbda, penalty = pen, \\\n",
    "                    monitor_evaluation_cost = True, \\\n",
    "                    monitor_evaluation_accuracy = False, \\\n",
    "                    monitor_training_cost = True, \\\n",
    "                    monitor_training_accuracy = False, \\\n",
    "                    monitor_roc = True, monitor_roc_test = True)\n",
    "                    \n",
    "                temp = pd.DataFrame({'penalty': [pen], \\\n",
    "                                         'lmbda': [lmbda], 'eta': [eta], \\\n",
    "                             'roc_train': [net_r.training_roc], \\\n",
    "                             'cost_train': [net_r.training_cost], \\\n",
    "                             'roc_test': [net_r.testing_roc], \\\n",
    "                             'cost_test': [net_r.evaluation_cost], 'run_id': [run_c]})\n",
    "                df_res = pd.concat([df_res, temp], axis=0)\n",
    "                    \n",
    "                for nb in df_res.run_id:\n",
    "                    avg = []\n",
    "                    for val in df_res[df_res.run_id == nb]['roc_test']:\n",
    "                        avg.append(np.mean(val))\n",
    "                    df_res.loc[df_res.run_id == nb, 'roc_test_avg'] = avg\n",
    "                    avg = []\n",
    "                    for val in df_res[df_res.run_id == nb]['roc_train']:\n",
    "                        avg.append(np.mean(val))\n",
    "                    df_res.loc[df_res.run_id == nb, 'roc_train_avg'] = avg\n",
    "                \n",
    "                df_res.to_csv(\"df_res.csv\")\n",
    "                \n",
    "                roc_current = net_r.testing_roc\n",
    "                if len(best_weights) == 0:\n",
    "                    best_weights = net_r.weights\n",
    "                    best_biases = net_r.biases\n",
    "                else:\n",
    "                    if roc_current > roc_avg:\n",
    "                        best_weights = net_r.weights\n",
    "                        best_biases = net_r.biases \n",
    "                \n",
    "                print \"step completed: {} / {}\".format(count, Nb_step)\n",
    "                count += 1\n",
    "                \n",
    "    #df_res.loc[df_res.run_id == nb, 'roc_test_avg_run'] = roc_test_run[nb-1]\n",
    "                    \n",
    "roc_test_run = df_res.groupby('run_id')['roc_test_avg'].agg(np.mean).tolist()\n",
    "for nb in Run_count:\n",
    "    df_res.loc[df_res.run_id == nb, 'roc_test_avg_run'] = roc_test_run[nb-1]\n",
    "\n",
    "df_res.to_csv(\"df_res.csv\")\n",
    "np.savetxt(\"previous_best_weights.csv\", best_weights, delimiter=\",\")\n",
    "np.savetxt(\"previous_best_biases.csv\", best_biases, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost_test</th>\n",
       "      <th>cost_train</th>\n",
       "      <th>eta</th>\n",
       "      <th>lmbda</th>\n",
       "      <th>penalty</th>\n",
       "      <th>roc_test</th>\n",
       "      <th>roc_test_avg</th>\n",
       "      <th>roc_test_avg_run</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_train_avg</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.195977352837, 0.195974512453]</td>\n",
       "      <td>[0.186149482041, 0.186145812965]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.751547533285, 0.751537527682]</td>\n",
       "      <td>0.751543</td>\n",
       "      <td>0.51955</td>\n",
       "      <td>[0.626982858361, 0.626979052491]</td>\n",
       "      <td>0.626981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          cost_test                        cost_train     eta  \\\n",
       "0  [0.195977352837, 0.195974512453]  [0.186149482041, 0.186145812965]  0.0001   \n",
       "\n",
       "   lmbda  penalty                          roc_test  roc_test_avg  \\\n",
       "0     10       16  [0.751547533285, 0.751537527682]      0.751543   \n",
       "\n",
       "   roc_test_avg_run                         roc_train  roc_train_avg  run_id  \n",
       "0           0.51955  [0.626982858361, 0.626979052491]       0.626981       2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "### Grid Search summary\n",
    "#############################################\n",
    "# Model where the best epoch occurred\n",
    "best_item = [max(x) for x in df_res.roc_test ]\n",
    "best_item = df_res[best_item == max(best_item)]\n",
    "best_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epochs\n",
      "Best roc test: 0.751547533285\n",
      "Best roc test: 0.751547533285, obtained after (array([0]),) / 2 epochs, for the model with eta = 0.0001, lmbda = 10.0, penalty = 16.0 on the 2.0 running time\n"
     ]
    }
   ],
   "source": [
    "print str(len(best_item.roc_test.tolist()[0])) + \" epochs\"\n",
    "ind_best = np.where(best_item.roc_test.tolist()[0] == max(best_item.roc_test.tolist()[0]))\n",
    "ind_worst = np.where(best_item.roc_test.tolist()[0] == min(best_item.roc_test.tolist()[0]))\n",
    "best_eta = best_item.eta.values[0]\n",
    "best_lmbda = best_item.lmbda.values[0]\n",
    "best_pen = best_item.penalty.values[0]\n",
    "best_run_id = best_item.run_id.values[0]\n",
    "print \"Best roc test: \" + str(max(best_item.roc_test.tolist()[0]))\n",
    "print \"Best roc test: {}, obtained after {} / {} epochs, for the model with eta = {}, \\\n",
    "lmbda = {}, penalty = {} on the {} running time\".format(\\\n",
    "max(best_item.roc_test.tolist()[0]), ind_best, len(best_item.roc_test.tolist()[0]), \\\n",
    "best_eta, best_lmbda, best_pen, best_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost_test</th>\n",
       "      <th>cost_train</th>\n",
       "      <th>eta</th>\n",
       "      <th>lmbda</th>\n",
       "      <th>penalty</th>\n",
       "      <th>roc_test</th>\n",
       "      <th>roc_test_avg</th>\n",
       "      <th>roc_test_avg_run</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_train_avg</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.195977352837, 0.195974512453]</td>\n",
       "      <td>[0.186149482041, 0.186145812965]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.751547533285, 0.751537527682]</td>\n",
       "      <td>0.751543</td>\n",
       "      <td>0.51955</td>\n",
       "      <td>[0.626982858361, 0.626979052491]</td>\n",
       "      <td>0.626981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          cost_test                        cost_train     eta  \\\n",
       "0  [0.195977352837, 0.195974512453]  [0.186149482041, 0.186145812965]  0.0001   \n",
       "\n",
       "   lmbda  penalty                          roc_test  roc_test_avg  \\\n",
       "0     10       16  [0.751547533285, 0.751537527682]      0.751543   \n",
       "\n",
       "   roc_test_avg_run                         roc_train  roc_train_avg  run_id  \n",
       "0           0.51955  [0.626982858361, 0.626979052491]       0.626981       2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with the best averaged roc test\n",
    "best_avg = df_res[df_res.roc_test_avg == max(df_res.roc_test_avg)]\n",
    "best_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost_test</th>\n",
       "      <th>cost_train</th>\n",
       "      <th>eta</th>\n",
       "      <th>lmbda</th>\n",
       "      <th>penalty</th>\n",
       "      <th>roc_test</th>\n",
       "      <th>roc_test_avg</th>\n",
       "      <th>roc_test_avg_run</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_train_avg</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.204901983917, 0.204885998622]</td>\n",
       "      <td>[0.21240566043, 0.212393686673]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.717977213043, 0.717984051866]</td>\n",
       "      <td>0.717981</td>\n",
       "      <td>0.47383</td>\n",
       "      <td>[0.721483744053, 0.721490028973]</td>\n",
       "      <td>0.721487</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          cost_test                       cost_train    eta  \\\n",
       "0  [0.204901983917, 0.204885998622]  [0.21240566043, 0.212393686673]  0.001   \n",
       "\n",
       "   lmbda  penalty                          roc_test  roc_test_avg  \\\n",
       "0     10       18  [0.717977213043, 0.717984051866]      0.717981   \n",
       "\n",
       "   roc_test_avg_run                         roc_train  roc_train_avg  run_id  \n",
       "0           0.47383  [0.721483744053, 0.721490028973]       0.721487       3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with the best averaged roc test among those for which roc_test_avg < roc_train_avg\n",
    "best_cons = df_res[df_res.roc_test_avg <= df_res.roc_train_avg]\n",
    "best_cons = best_cons[best_cons.roc_test_avg == max(best_cons.roc_test_avg)]\n",
    "best_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## best: eta = 0.0001, lmbda = 17.0, penalty = 16.0 : roc_test_avg_run = 0.466\n",
    "## best among test < train: eta = 0.01, lmbda = 9.0, penalty = 18.0 : roc_test_avg_run = 0.466\n",
    "\n",
    "## Bigger sample:\n",
    "## best: eta = 0.01, lmbda = 22.0, penalty = 16.0 : roc_test_avg_run = 0.466\n",
    "## best among test < train: eta = 0.01, lmbda = 9.0, penalty = 18.0 : roc_test_avg_run = 0.466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best previous weights\n",
      "[0.68149845677146048, 0.69111586446078221, 0.69771095309966524, 0.71547341004713882, 0.71775129813257166, 0.71889381425277221, 0.72022644275685954, 0.72119097425401368, 0.72190298787593643, 0.72206691413157387, 0.72235894114170673, 0.72266618761518076, 0.72299064790500778, 0.72341035221112004, 0.72353757189870271, 0.72376464355975867, 0.72401727299126573, 0.72433191405216402, 0.72454992688829356, 0.72491685824597774, 0.72511789176159913, 0.72536215219576261, 0.72561335188561249, 0.72569128874253841, 0.72599256809358648, 0.72659921339803069, 0.72746910191679359, 0.72695242788710912, 0.72741489922670821, 0.72905518855615115, 0.7295032304329746, 0.73097400430969672, 0.73272067962305232, 0.73448617607379618, 0.73627039121228677, 0.73764253795791079, 0.73965092429428669, 0.74077910657137691, 0.74138574732250362, 0.74256869234771661, 0.7441538683235982, 0.74469965398794824, 0.74598721375661348, 0.7467479205545926, 0.74831473072482413, 0.74912107769955627, 0.75042191266504132, 0.75206205856498676, 0.75317404241549912, 0.75510686417330408]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "### Train the best model\n",
    "#############################################\n",
    "net_b = Network([len(training_data[0][0]), 100, 10, 1], previous_best_weights, \\\n",
    "                previous_best_biases)\n",
    "net_b.SGD(training_data=training_data, \\\n",
    "          evaluation_data = None, \\\n",
    "                    epochs=50, mini_batch_size=500, \\\n",
    "                    eta=1e-1, lmbda = 10, penalty = 18, \\\n",
    "                    monitor_evaluation_cost = False, \\\n",
    "                    monitor_evaluation_accuracy = False, \\\n",
    "                    monitor_training_cost = True, \\\n",
    "                    monitor_training_accuracy = False, \\\n",
    "                    monitor_roc = True, monitor_roc_test = False)\n",
    "print net_b.training_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "previous_best_weights = best_weights\n",
    "previous_best_biases = best_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
