{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Neural Network for Santander\n",
    "~~~~~~~~~~~~~~\n",
    "We implement the stochastic gradient descent learning algorithm\n",
    "for a feedforward neural network. ~4% of classes are 1s, 96% are 0s.\n",
    "Improvements for class imbalance (outlier detection)\n",
    "include the addition of the cross-entropy cost function,\n",
    "regularization, and better initialization of network weights. \n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \"\"\"\n",
    "        penalty=14.5\n",
    "        if y==1 and a < 0.5: fun = penalty*0.5*np.linalg.norm(a-y)**2\n",
    "        else: fun = 1*0.5*np.linalg.norm(a-y)**2\n",
    "        return fun\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=QuadraticCost):\n",
    "        \"\"\" list ``sizes`` [a,b,c...] represents a multilayer network\n",
    "        with b,c,d... hidden neurons in each layer.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the ***square root of the number of\n",
    "        weights*** connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1. First layer has no biases. \n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. \n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "#             train_norm0 = [(x,y) for (x,y) in training_data if y==0]\n",
    "#             train_norm1 = [(x,y) for (x,y) in training_data if y==1]\n",
    "#             train_norm0 = random.sample(train_norm0, len(train_norm1))\n",
    "#             training_data = train_norm0+train_norm1\n",
    "            random.shuffle(training_data)\n",
    "#             print sum([y==1 for (x,y) in training_data])\n",
    "#             print n\n",
    "#             print np.mean(test_data==1)\n",
    "#             print np.mean((y==1 for (x,y) in training_data))\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, len(training_data), mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            weightsdict = {'0':1, '1':14.5}\n",
    "            nabla_b = [nb + weightsdict[str(int(y))]*dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + weightsdict[str(int(y))]*dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "\n",
    "        \"\"\"\n",
    "        results = [(weightedround(self.feedforward(x)), y) for (x, y) in data]\n",
    "        lst = [self.feedforward(x)[0][0] for (x, y) in data]\n",
    "        print np.unique(lst)\n",
    "            \n",
    "        return sum(x == y for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda):\n",
    "        \"\"\"Return the total cost for data.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if y == 1 and a < 0.5:\n",
    "                cost += self.cost.fn(a, y)/len(data)\n",
    "            else:\n",
    "                cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    def predict(self, data):\n",
    "        res = [self.feedforward(x)[0][0] for x in data]\n",
    "        return res\n",
    "    \n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Other functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def weightedround(x):\n",
    "    return 0 if x<0.45 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "train=pd.read_csv(\"/Users/bohun/Documents/DataSci/santandertrain.csv\", sep=',')\n",
    "train = train.set_index(\"ID\")\n",
    "x = train.values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "trainnorm = pd.DataFrame(x_scaled, columns = train.columns, index = train.index)\n",
    "# trainnorm.to_csv(\"/Users/bohun/Documents/kaggleproject/trainnorm.csv\", sep=',', encoding='utf-8')\n",
    "labels = trainnorm['TARGET'].set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainfilt = pd.read_csv(\"/Users/bohun/Documents/kaggleproject/filteredtrain.csv\", sep=',',\\\n",
    "                        encoding='utf-8', dtype = np.float64, index_col = 0)\n",
    "y = trainfilt.values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "y_scaled = min_max_scaler.fit_transform(y)\n",
    "trainfilt = pd.DataFrame(y_scaled, columns = trainfilt.columns, index = trainfilt.index)\n",
    "# trainfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trainarrayfilt = trainfilt.values\n",
    "trainarrayfilt = [np.reshape(x, (282, 1)) for x in trainarrayfilt]\n",
    "training_datafilt = zip(trainarrayfilt, labels.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21020 55000\n",
      "2129.0\n",
      "879.0\n"
     ]
    }
   ],
   "source": [
    "trainfilt = training_datafilt[0:55000]\n",
    "testfilt = training_datafilt[55000:76020]\n",
    "\n",
    "print len(testfilt), len(trainfilt)\n",
    "print np.sum([j for (i,j) in trainfilt if j==1])\n",
    "print np.sum([j for (i,j) in testfilt if j==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trainnorm = pd.read_csv(\"/Users/bohun/Documents/kaggleproject/trainnorm.csv\", sep=',', encoding='utf-8').set_index(\"ID\")\n",
    "trainarray2 = trainnorm.iloc[:,0:369].values #for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#balanced training set\n",
    "\n",
    "# subset for TARGET = 1\n",
    "train1 = trainnorm[trainnorm.TARGET == 1]\n",
    "ind_train1 = random.sample(range(len(train1)), int(0.8*len(train1)))\n",
    "ind_test1 = [i for i in range(len(train1)) if i not in ind_train1]\n",
    "train1Train = train1.iloc[ind_train1, :]\n",
    "train1Test = train1.iloc[ind_test1, :]\n",
    "\n",
    "# subset for TARGET = 0\n",
    "train0 = trainnorm[trainnorm.TARGET == 0]\n",
    "ind_train0 = random.sample(range(len(train0)), int(0.8*len(train1)))\n",
    "ind_test0 = random.sample([i for i in range(len(train0)) if i not in ind_train0],\\\n",
    "                          int(0.2*len(train1)))\n",
    "train0Train = train0.iloc[ind_train0, :]\n",
    "train0Test = train0.iloc[ind_test0, :]\n",
    "\n",
    "trainnormsub = pd.concat((train0Train, train1Train), axis=0)\n",
    "trainarray = trainnormsub.iloc[:,0:369].values\n",
    "trainarray = [np.reshape(x, (369, 1)) for x in trainarray]\n",
    "traintarget = trainnormsub.iloc[:,369].values.T\n",
    "\n",
    "testnormsub = pd.concat([train0Test, train1Test], axis=0)\n",
    "testarray = testnormsub.iloc[:,0:369].values\n",
    "testarray = [np.reshape(x, (369, 1)) for x in testarray]\n",
    "testtarget = testnormsub.iloc[:,369].values.T\n",
    "\n",
    "test_data = zip(testarray, testtarget)\n",
    "train_data = zip(trainarray, traintarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imbalanced training, test set\n",
    "import numpy as np\n",
    "trainarray = trainnorm.iloc[:,0:369].values\n",
    "trainarray = [np.reshape(x, (369, 1)) for x in trainarray]\n",
    "target = trainnorm.iloc[:,369].values.T\n",
    "training_data = zip(trainarray, target)\n",
    "train_data = training_data\n",
    "# test_data = training_data[60816:76020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0395685345962\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-534ac439149b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainnormsub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "print np.mean([y==1 for (x,y) in train_data])\n",
    "print np.mean([y==1 for (x,y) in test_data])\n",
    "print np.unique(trainnormsub.TARGET)\n",
    "print len(test_data)\n",
    "print len(train_data)\n",
    "print len(trainarray)\n",
    "print len(testarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0395685345962\n",
      "(76020, 370)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainnorm.head()\n",
    "print np.mean(target==1)\n",
    "print trainnorm.shape\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainnorm[trainnorm.TARGET == 1].shape\n",
    "\n",
    "net.save(\"282100151etae3lbmda100.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 0.185117556407\n",
      "[ 0.46590259  0.46597111  0.46597528 ...,  0.47460229  0.47472102\n",
      "  0.47496223]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.46574669  0.46581586  0.46624725 ...,  0.47449484  0.47465987\n",
      "  0.47467746]\n",
      "[ 0.46574669  0.46581586  0.46624725 ...,  0.47449484  0.47465987\n",
      "  0.47467746]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.184182515664\n",
      "[ 0.4597397   0.45978888  0.45982046 ...,  0.46841774  0.46855301\n",
      "  0.46879041]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.45959135  0.45966344  0.46010062 ...,  0.46831782  0.4684911   0.4685153 ]\n",
      "[ 0.45959135  0.45966344  0.46010062 ...,  0.46831782  0.4684911   0.4685153 ]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.183359215228\n",
      "[ 0.45396984  0.45400086  0.45405815 ...,  0.46262522  0.46277583\n",
      "  0.46300967]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.45382864  0.45390344  0.4543458  ...,  0.46253214  0.4627133\n",
      "  0.46274361]\n",
      "[ 0.45382864  0.45390344  0.4543458  ...,  0.46253214  0.4627133\n",
      "  0.46274361]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.182633486271\n",
      "[ 0.44856302  0.44857698  0.4486584  ...,  0.45719508  0.45735994\n",
      "  0.45759044]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.44842857  0.4485059   0.44895291 ...,  0.4571082   0.45729692\n",
      "  0.45733286]\n",
      "[ 0.44842857  0.4485059   0.44895291 ...,  0.4571082   0.45729692\n",
      "  0.45733286]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.181993295925\n",
      "[ 0.4434915   0.44349358  0.44359558 ...,  0.45210362  0.45228007\n",
      "  0.45250743]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.44336551  0.44344519  0.44389638 ...,  0.45202072  0.45221668\n",
      "  0.45225783]\n",
      "[ 0.44336551  0.44344519  0.44389638 ...,  0.45202072  0.45221668\n",
      "  0.45225783]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.18142899429\n",
      "[ 0.43872796  0.43874507  0.43885327 ...,  0.44733651  0.44752013\n",
      "  0.44774457]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.43862305  0.43870492  0.43915985 ...,  0.44725357  0.44745648\n",
      "  0.44750243]\n",
      "[ 0.43862305  0.43870492  0.43915985 ...,  0.44725357  0.44745648\n",
      "  0.44750243]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.18093111532\n",
      "[ 0.43426279  0.43429401  0.43440802 ...,  0.44286825  0.4430569   0.4432786 ]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.43417769  0.4342616   0.43471988 ...,  0.4427835   0.44299311\n",
      "  0.44304347]\n",
      "[ 0.43417769  0.4342616   0.43471988 ...,  0.4427835   0.44299311\n",
      "  0.44304347]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.180490830298\n",
      "[ 0.43006833  0.43011284  0.4302323  ...,  0.43867274  0.43886303\n",
      "  0.43908215]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.43000191  0.43008773  0.43054902 ...,  0.43858312  0.43879919\n",
      "  0.43885363]\n",
      "[ 0.43000191  0.43008773  0.43054902 ...,  0.43858312  0.43879919\n",
      "  0.43885363]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.180102339484\n",
      "[ 0.42613762  0.42619457  0.42631914 ...,  0.43473985  0.43493179\n",
      "  0.43514851]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.42608874  0.42617635  0.42664033 ...,  0.43464568  0.434868    0.43492618]\n",
      "[ 0.42608874  0.42617635  0.42664033 ...,  0.43464568  0.434868    0.43492618]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.179758837399\n",
      "[ 0.42244717  0.42251583  0.42264521 ...,  0.43104633  0.43123993\n",
      "  0.43145438]\n",
      "Accuracy on training data: 52871 / 55000\n",
      "[ 0.42241482  0.42250411  0.4229705  ...,  0.4309479   0.43117626\n",
      "  0.43123788]\n",
      "[ 0.42241482  0.42250411  0.4229705  ...,  0.4309479   0.43117626\n",
      "  0.43123788]\n",
      "Accuracy on evaluation data: 20141 / 21020\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [20141, 20141, 20141, 20141, 20141, 20141, 20141, 20141, 20141, 20141],\n",
       " [0.18511755640728264,\n",
       "  0.1841825156637468,\n",
       "  0.18335921522820497,\n",
       "  0.18263348627146672,\n",
       "  0.18199329592532287,\n",
       "  0.18142899428998968,\n",
       "  0.18093111531960585,\n",
       "  0.18049083029751198,\n",
       "  0.18010233948423682,\n",
       "  0.17975883739949297],\n",
       " [52871, 52871, 52871, 52871, 52871, 52871, 52871, 52871, 52871, 52871])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([282, 100, 15, 1])\n",
    "net.SGD(training_data=trainfilt, evaluation_data = testfilt, epochs=10, mini_batch_size=500, eta=1e-3, \\\n",
    "monitor_evaluation_cost = False, \\\n",
    "monitor_evaluation_accuracy = True, \\\n",
    "monitor_training_cost = True, \\\n",
    "monitor_training_accuracy = True, lmbda = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #  That's better! And so we can continue, individually adjusting each hyper-parameter,\n",
    "# gradually improving performance. Once we've explored to find an improved value for ηη, \n",
    "# then we move on to find a good value for λλ. Then experiment with a more complex architecture,\n",
    "# say a network with 10 hidden neurons. Then adjust the values for ηη and λλ again. Then increase to 20 hidden neurons. \n",
    "# And then adjust other hyper-parameters some more. And so on, at each stage evaluating performance using our \n",
    "# held-out validation data, and using those evaluations to find better and better hyper-parameters. As we do so, \n",
    "# it typically takes longer to witness the impact due to modifications of the hyper-parameters, and so we can \n",
    "# gradually decrease the frequency of monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainarray2 = trainnorm.iloc[:,0:369].values\n",
    "target = trainnorm.iloc[:,369].values.T\n",
    "len(trainarray2)==len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71299999999999997"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #clf.support_vectors_\n",
    "from sklearn import svm\n",
    "trainarray2 = trainnorm.iloc[:,0:369].values\n",
    "target = trainnorm.iloc[:,369].values.T\n",
    "len(trainarray2)==len(target)\n",
    "\n",
    "\n",
    "X = trainarray2[1000:10000]\n",
    "y = target[1000:10000]\n",
    "clf = svm.SVC(class_weight={1:20}, C=1.0, kernel = 'rbf')\n",
    "clf.fit(X, y)  \n",
    "# SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#     decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "#     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#     tol=0.001, verbose=False)\n",
    "len(clf.support_vectors_)\n",
    "np.mean([clf.predict(trainarray2[10000:12000])==target[10000:12000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0395685345962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76020, 369)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainarray2 = trainnorm.iloc[:,0:369].values\n",
    "print np.mean(target == 1)\n",
    "trainarray2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X = trainarray2[1:1000]\n",
    "y = target[1:1000]\n",
    "clf = svm.SVC(class_weight={1:2}, C=3, kernel = 'linear')\n",
    "clf.fit(X, y) \n",
    "print len(clf.support_vectors_)\n",
    "clf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.,  0.,  5.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(np.around(net.feedforward(train_data[1][0]) ), y) for (x, y) in train_data]\n",
    "#clf.coef_\n",
    "np.linspace(-5, 5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 369)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainarray2[1:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58468"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [(round(net.feedforward(train_data[1][0])), y) for (x, y) in train_data]\n",
    "\n",
    "sum(x == y for (x, y) in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21667452]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.feedforward(train_data[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5269.4576654855"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5269.4576654855)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
